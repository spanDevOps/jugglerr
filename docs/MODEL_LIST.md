# Complete Model List

**Date:** Sunday, December 7, 2025  
**Total Models:** 115

---

## Summary by Provider

| Provider | Models | Free Tier | Streaming |
|----------|--------|-----------|-----------|
| NVIDIA | 56 | ‚úÖ Yes | ‚úÖ Yes |
| Mistral | 19 | Trial | ‚úÖ Yes |
| Groq | 18 | ‚úÖ Yes | ‚úÖ Yes |
| Cohere | 10 | ‚úÖ Yes | ‚úÖ Yes |
| Cerebras | 6 | ‚úÖ Yes | ‚úÖ Yes |
| nvidia-embed | 6 | ‚úÖ Yes | N/A |
| **Total** | **115** | - | **109/109** |

---

## Cerebras (6 models) - FREE TIER ‚úÖ

All models support streaming ‚úÖ

1. **zai-glm-4.6** [super] [64K context]
   - 357B total (32B active)
   - Dual-mode reasoning
   - ~1000 tokens/sec

2. **qwen-3-235b-a22b-instruct-2507** [super] [65K context]
   - World's fastest frontier reasoning
   - ~1400-1700 tokens/sec

3. **gpt-oss-120b** [super] [medium context]
   - Open-source GPT alternative
   - Strong reasoning

4. **llama-3.3-70b** [super] [medium context]
   - Meta's latest
   - Excellent quality

5. **qwen-3-32b** [regular] [medium context]
   - Fast and efficient
   - Good for general tasks

6. **llama3.1-8b** [regular] [small context]
   - Fastest model
   - Great for simple tasks

---

## Groq (18 models) - FREE TIER ‚úÖ

All models support streaming ‚úÖ

### Super Models
1. **moonshotai/kimi-k2-instruct-0905** [super] [128K context]
2. **openai/gpt-oss-120b** [super] [128K context]
3. **llama-3.3-70b-versatile** [super] [128K context]
4. **llama-3.3-70b-specdec** [super] [8K context]
5. **llama-3.1-70b-versatile** [super] [128K context]
6. **llama-3.1-70b-specdec** [super] [8K context]

### Regular Models
7. **meta-llama/llama-4-maverick-17b-128e-instruct** [regular] [131K context] - Vision
8. **meta-llama/llama-4-scout-17b-16e-instruct** [regular] [131K context] - Vision
9. **qwen/qwen3-32b** [regular] [128K context]
10. **openai/gpt-oss-20b** [regular] [128K context]
11. **llama-3.1-8b-instant** [regular] [128K context]
12. **llama3-groq-70b-8192-tool-use-preview** [regular] [8K context]
13. **llama3-groq-8b-8192-tool-use-preview** [regular] [8K context]
14. **llama-3.2-1b-preview** [regular] [8K context]
15. **llama-3.2-3b-preview** [regular] [8K context]
16. **llama-3.2-11b-vision-preview** [regular] [8K context] - Vision
17. **llama-3.2-90b-vision-preview** [regular] [8K context] - Vision
18. **gemma2-9b-it** [regular] [8K context]

---

## Mistral (19 models) - TRIAL CREDITS

All models support streaming ‚úÖ

### Large Models (Super)
1. **mistral-large-2411** [super] [128K context]
2. **mistral-medium-2508** [super] [128K context]
3. **magistral-medium-2507** [super] [128K context]
4. **pixtral-large-2411** [super] [128K context] - Vision
5. **devstral-medium-2507** [super] [medium context]

### Regular Models
6. **codestral-2501** [regular] [256K context]
7. **magistral-small-2507** [regular] [128K context]
8. **mistral-small-2409** [regular] [128K context]
9. **pixtral-12b-2409** [regular] [128K context] - Vision
10. **devstral-small-2505** [regular] [medium context]
11. **open-mistral-nemo** [regular] [128K context]
12. **ministral-8b-2410** [regular] [128K context]
13. **ministral-3b-2410** [regular] [128K context]
14. **mistral-embed** [regular] - Embeddings
15. **codestral-embed** [regular] - Code Embeddings
16. **mistral-ocr-latest** [regular] - OCR
17. **mistral-moderation-latest** [regular] - Moderation
18. **pixtral-ocr-2412** [regular] - OCR
19. **mistral-moderation-2412** [regular] - Moderation

---

## Cohere (10 models) - FREE TIER ‚úÖ

All chat models support streaming ‚úÖ

### Chat Models
1. **command-r-plus-08-2024** [super] [128K context]
2. **command-r-plus** [super] [128K context]
3. **command-r-08-2024** [regular] [128K context]
4. **command-r** [regular] [128K context]
5. **command** [regular] [4K context]

### Specialized Models
6. **embed-english-v3.0** - Embeddings
7. **embed-multilingual-v3.0** - Multilingual Embeddings
8. **embed-v4.0** - Latest Embeddings
9. **rerank-v3.5** - Reranking
10. **rerank-english-v3.0** - English Reranking

---

## NVIDIA (56 models) - FREE TIER ‚úÖ

All models support streaming ‚úÖ

### Super Models (70B+)
1. **qwen/qwen3-235b-a22b** [235B]
2. **qwen/qwen3-coder-480b-a35b-instruct** [480B]
3. **qwen/qwen3-next-80b-a3b-instruct** [80B]
4. **deepseek-ai/deepseek-v3.1** [671B]
5. **deepseek-ai/deepseek-v3.1-terminus** [671B]
6. **stockmark/stockmark-2-100b-instruct** [100B]
7. **abacusai/dracarys-llama-3.1-70b-instruct** [70B]
8. **bytedance/seed-oss-36b-instruct** [36B]

### Vision Models
9. **microsoft/phi-3.5-vision-instruct**
10. **microsoft/phi-4-multimodal-instruct**
11. **nvidia/vila**
12. **meta/llama-4-maverick-17b-128e-instruct**
13. **meta/llama-4-scout-17b-16e-instruct**

### Reasoning Models
14. **microsoft/phi-4-mini-flash-reasoning**
15. **deepseek-ai/deepseek-r1-0528**
16. **qwen/qwq-32b**

### Code Specialists
17. **qwen/qwen2.5-coder-7b-instruct**
18. **mistralai/mamba-codestral-7b-v0.1**

### Multilingual Models
19. **nvidia/nemotron-4-mini-hindi-4b-instruct**
20. **nvidia/riva-translate-4b-instruct**
21. **speakleash/bielik-11b-v2.6-instruct** (Polish)
22. **igenius/italia_10b_instruct_16k** (Italian)
23. **marin/marin-8b-instruct** (Japanese)
24. **mediatek/breeze-7b-instruct** (Chinese)

### Regular Models (7B-32B)
25. **google/gemma-3-27b-it**
26. **google/gemma-3n-e4b-it**
27. **google/gemma-3n-e2b-it**
28. **google/gemma-2-27b-it**
29. **google/gemma-2-2b-it**
30. **google/gemma-7b**
31. **google/shieldgemma-9b**
32. **microsoft/phi-3-medium-128k-instruct**
33. **microsoft/phi-3-medium-4k-instruct**
34. **microsoft/phi-3-small-128k-instruct**
35. **microsoft/phi-3-small-8k-instruct**
36. **microsoft/phi-3.5-mini-instruct**
37. **mistralai/mistral-medium-3-instruct**
38. **mistralai/mistral-small-3.1-24b-instruct-2503**
39. **mistralai/magistral-small-2506**
40. **mistralai/mistral-nemotron**
41. **moonshotai/kimi-k2-instruct**
42. **moonshotai/kimi-k2-instruct-0905**
43. **minimaxai/minimax-m2**
44. **ai21labs/jamba-1.5-mini-instruct**
45. **baichuan-inc/baichuan2-13b-chat**
46. **ibm/granite-3.3-8b-instruct**
47. **ibm/granite-guardian-3.0-8b**
48. **nvidia/llama3-chatqa-1.5-8b**
49. **nvidia/nemotron-mini-4b-instruct**
50. **qwen/qwen2-7b-instruct**
51. **rakuten/rakutenai-7b-chat**
52. **rakuten/rakutenai-7b-instruct**
53. **thudm/chatglm3-6b**
54. **tiiuae/falcon3-7b-instruct**
55. **upstage/solar-10.7b-instruct**
56. **meta/llama-guard-4-12b** (Safety model)

---

## Model Selection Guide

### For Speed üöÄ
- **Cerebras:** llama3.1-8b, qwen-3-32b
- **Groq:** llama-3.1-8b-instant
- **Mistral:** ministral-3b-2410, ministral-8b-2410

### For Quality üéØ
- **Cerebras:** zai-glm-4.6, qwen-3-235b
- **Groq:** kimi-k2, gpt-oss-120b
- **Mistral:** mistral-large-2411
- **NVIDIA:** deepseek-v3.1, qwen3-235b

### For Vision üëÅÔ∏è
- **Groq:** llama-4-maverick, llama-4-scout
- **Mistral:** pixtral-large-2411, pixtral-12b-2409
- **NVIDIA:** phi-3.5-vision, phi-4-multimodal, vila

### For Reasoning üß†
- **Cerebras:** zai-glm-4.6, qwen-3-235b
- **Groq:** kimi-k2
- **Mistral:** magistral-medium, magistral-small
- **NVIDIA:** deepseek-r1, qwq-32b, phi-4-mini-flash

### For Code üíª
- **Mistral:** codestral-2501, devstral-medium
- **NVIDIA:** qwen2.5-coder, mamba-codestral

### For Large Context üìö
- **Groq:** All models (128K+)
- **Mistral:** codestral-2501 (256K)
- **NVIDIA:** Most models (128K+)

### For Free Tier üÜì
- **Cerebras:** All 6 models
- **Groq:** All 8 models
- **Cohere:** All 5 models
- **NVIDIA:** All 56 models

---

## Streaming Support

### Fully Supported ‚úÖ
- **Cerebras:** All 6 chat models
- **Groq:** All 18 chat models
- **Mistral:** 13 chat models (custom JSON lines format)
- **Cohere:** 5 chat models (custom JSON lines format)
- **NVIDIA:** All 56 chat models

**Total Streaming:** 109/109 chat models (100%)

### No Streaming (Specialized Models)
- Embedding models (6 nvidia-embed models)
- Mistral specialized models (6 models)
- Cohere specialized models (5 models)

---

## Context Windows

### Large (100K+)
- **Groq:** All models (128K-131K)
- **Mistral:** Most models (128K-256K)
- **NVIDIA:** Most models (128K+)

### Medium (30K-100K)
- **Cerebras:** Most models (64K-65K)
- **Cohere:** command-r models (128K)

### Small (<30K)
- **Cerebras:** llama3.1-8b
- **Cohere:** command (4K)

---

## Power Levels

### Super (70B+)
- **Cerebras:** 4 models
- **Groq:** 6 models
- **Mistral:** 5 models
- **Cohere:** 2 models
- **NVIDIA:** 8 models
- **Total:** 25 super models

### Regular (7B-32B)
- **Cerebras:** 2 models
- **Groq:** 12 models
- **Mistral:** 8 models
- **Cohere:** 3 models
- **NVIDIA:** 48 models
- **Total:** 73 regular models

### Specialized
- **Mistral:** 6 models (embeddings, OCR, moderation)
- **Cohere:** 5 models (embeddings, reranking)
- **nvidia-embed:** 6 models (embeddings)
- **Total:** 17 specialized models

---

## Usage Examples

```python
from Jugglerr import LLMJuggler

Jugglerr = LLMJuggler()

# Use fastest model
response = Jugglerr.juggle(
    messages=[{"role": "user", "content": "Quick question"}],
    preferred_provider="groq",
    preferred_model="llama-3.1-8b-instant"
)

# Use best quality
response = Jugglerr.juggle(
    messages=[{"role": "user", "content": "Complex analysis"}],
    power="super",
    preferred_provider="cerebras",
    preferred_model="qwen-3-235b-a22b-instruct-2507"
)

# Use vision model
response = Jugglerr.juggle(
    messages=[{"role": "user", "content": "Describe this image"}],
    capabilities=["vision"],
    preferred_provider="groq"
)

# Stream response
for chunk in Jugglerr.juggle_stream(
    messages=[{"role": "user", "content": "Tell me a story"}],
    preferred_provider="cerebras"
):
    print(chunk, end='', flush=True)
```

## nvidia-embed (6 models) - FREE TIER ‚úÖ

Embedding models (no streaming)

1. **nvidia/nv-embed-v1** [4096 dims]
2. **nvidia/nv-embed-v2** [4096 dims]
3. **nvidia/nv-embedqa-e5-v5** [1024 dims]
4. **nvidia/nv-embedqa-mistral-7b-v2** [4096 dims]
5. **baai/bge-m3** [1024 dims] - Multilingual
6. **snowflake/arctic-embed-l** [1024 dims]

---

**Last Updated:** December 7, 2025  
**Total Models:** 115  
**Chat Models with Streaming:** 109/109 (100%)  
**Specialized Models:** 6 (embeddings, reranking, OCR, moderation)
